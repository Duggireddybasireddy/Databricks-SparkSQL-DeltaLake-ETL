# Databricks-SparkSQL-DeltaLake-ETL
## Introduction

This repository showcases a complete **end-to-end data engineering project** built using **Databricks, Spark SQL, and Delta Lake**. It demonstrates how raw data can be ingested, transformed, stored, and queried efficiently using modern big data technologies and the **Lakehouse architecture**.

The main purpose of this project is to provide a **practical, hands-on learning experience** for engineers, students, and professionals who want to build expertise in:

- Big Data Processing  
- Distributed Computing with Spark  
- SQL-based ETL Pipelines  
- Working with real-world datasets  

The project uses the **NYC Taxi dataset**, which is widely used in industry and academia for benchmarking, analytics, and ETL workflows. By following the notebooks provided in this repository, users can learn how to:

- Ingest large datasets into Databricks  
- Clean and process data using SQL  
- Store data in the Delta Lake format with ACID transactions  
- Run analytical queries to extract insights  

Whether you're preparing for a **data engineering job**, building a **portfolio project**, or simply exploring Databricks and Spark, this project offers a valuable opportunity to gain **hands-on experience with modern industry tools and workflows**.

## About Datasets

- Their are two types of taxi's ecspecially in **NEW YORK CITY(NYC)**. This taxi's are **YELLOW** and **GREEN**.
- This two datasets are used in this project.
